{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 특정 GPU에 1GB 메모리만 할당하도록 제한\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7500)])\n",
    "    except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "        print(e)\n",
    "        \n",
    "print('GPU ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wine': 0, 'soju': 1, 'plate': 2, 'me': 3, 'glass': 4, 'energydrink': 5, 'case': 6, 'bottle': 7, 'be': 8}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "category_csv = pd.read_csv('./category/category.csv', encoding='CP949')\n",
    "label_dict = dict(category_csv[['bottle_name', 'bottle_id']].values)\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 299, 299, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 299, 299, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 149, 149, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 149, 149, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 149, 149, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 74, 74, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 74, 74, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 74, 74, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 74, 74, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "with tf.device('/device:GPU:0'):\n",
    "# VGG16은 Convolution Layer 16개, Fc Layer는 3개로 구성된 Network\n",
    "    base_model = VGG16(weights= 'imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(299,299,3))\n",
    "\n",
    "    base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 9 classes.\n",
      "Found 20 images belonging to 9 classes.\n",
      "100\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "\n",
    "rootPath = './image_data/촬영본'\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    train_imageGenerator = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=.2\n",
    "    )\n",
    "    \n",
    "    val_imageGenerator = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=.2\n",
    "    )\n",
    "    \n",
    "    trainGen = train_imageGenerator.flow_from_directory(\n",
    "        rootPath,\n",
    "        target_size=(299, 299),\n",
    "        classes=label_dict,\n",
    "        subset='training',\n",
    "        batch_size = 100,\n",
    "        class_mode=\"sparse\"\n",
    "    )\n",
    "\n",
    "    validationGen = val_imageGenerator.flow_from_directory(\n",
    "        rootPath,\n",
    "        target_size=(299, 299),\n",
    "        classes=label_dict,\n",
    "        subset='validation',\n",
    "        batch_size = 20,\n",
    "        class_mode=\"sparse\"\n",
    "    )\n",
    "    train_batch_size =100\n",
    "    val_batch_size =20\n",
    "\n",
    "\n",
    "    print(len(trainGen.classes))\n",
    "    print(len(validationGen.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 9, 9, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9)                 373257    \n",
      "=================================================================\n",
      "Total params: 15,087,945\n",
      "Trainable params: 373,257\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:5 out of the last 40 calls to <function Model.make_train_function.<locals>.train_function at 0x7f70b4130560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3663 - sparse_categorical_accuracy: 0.1500WARNING:tensorflow:5 out of the last 40 calls to <function Model.make_test_function.<locals>.test_function at 0x7f70b5599dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.13295, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 1s 682ms/step - loss: 2.3663 - sparse_categorical_accuracy: 0.1500 - val_loss: 2.1329 - val_sparse_categorical_accuracy: 0.2000\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1000 - sparse_categorical_accuracy: 0.2100\n",
      "Epoch 00002: val_loss improved from 2.13295 to 1.86171, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 2.1000 - sparse_categorical_accuracy: 0.2100 - val_loss: 1.8617 - val_sparse_categorical_accuracy: 0.3500\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8467 - sparse_categorical_accuracy: 0.4200\n",
      "Epoch 00003: val_loss improved from 1.86171 to 1.74800, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 1.8467 - sparse_categorical_accuracy: 0.4200 - val_loss: 1.7480 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7721 - sparse_categorical_accuracy: 0.4700\n",
      "Epoch 00004: val_loss improved from 1.74800 to 1.68197, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 1.7721 - sparse_categorical_accuracy: 0.4700 - val_loss: 1.6820 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7389 - sparse_categorical_accuracy: 0.3600\n",
      "Epoch 00005: val_loss improved from 1.68197 to 1.60623, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 1.7389 - sparse_categorical_accuracy: 0.3600 - val_loss: 1.6062 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6631 - sparse_categorical_accuracy: 0.4800\n",
      "Epoch 00006: val_loss improved from 1.60623 to 1.56718, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 1.6631 - sparse_categorical_accuracy: 0.4800 - val_loss: 1.5672 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5932 - sparse_categorical_accuracy: 0.5000\n",
      "Epoch 00007: val_loss improved from 1.56718 to 1.50086, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 1.5932 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.5009 - val_sparse_categorical_accuracy: 0.5500\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5526 - sparse_categorical_accuracy: 0.6000\n",
      "Epoch 00008: val_loss improved from 1.50086 to 1.44576, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 1.5526 - sparse_categorical_accuracy: 0.6000 - val_loss: 1.4458 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5044 - sparse_categorical_accuracy: 0.5600\n",
      "Epoch 00009: val_loss improved from 1.44576 to 1.40334, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 1.5044 - sparse_categorical_accuracy: 0.5600 - val_loss: 1.4033 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4956 - sparse_categorical_accuracy: 0.6200\n",
      "Epoch 00010: val_loss improved from 1.40334 to 1.37354, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 1.4956 - sparse_categorical_accuracy: 0.6200 - val_loss: 1.3735 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4077 - sparse_categorical_accuracy: 0.6600\n",
      "Epoch 00011: val_loss improved from 1.37354 to 1.33395, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 1.4077 - sparse_categorical_accuracy: 0.6600 - val_loss: 1.3340 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3782 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 00012: val_loss improved from 1.33395 to 1.29514, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 1.3782 - sparse_categorical_accuracy: 0.7000 - val_loss: 1.2951 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3820 - sparse_categorical_accuracy: 0.6600\n",
      "Epoch 00013: val_loss improved from 1.29514 to 1.26770, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 1.3820 - sparse_categorical_accuracy: 0.6600 - val_loss: 1.2677 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3221 - sparse_categorical_accuracy: 0.6800\n",
      "Epoch 00014: val_loss improved from 1.26770 to 1.23885, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 1.3221 - sparse_categorical_accuracy: 0.6800 - val_loss: 1.2389 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2633 - sparse_categorical_accuracy: 0.6900\n",
      "Epoch 00015: val_loss improved from 1.23885 to 1.19760, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 1.2633 - sparse_categorical_accuracy: 0.6900 - val_loss: 1.1976 - val_sparse_categorical_accuracy: 0.8500\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2332 - sparse_categorical_accuracy: 0.7400\n",
      "Epoch 00016: val_loss improved from 1.19760 to 1.18477, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 1.2332 - sparse_categorical_accuracy: 0.7400 - val_loss: 1.1848 - val_sparse_categorical_accuracy: 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2467 - sparse_categorical_accuracy: 0.7100\n",
      "Epoch 00017: val_loss improved from 1.18477 to 1.14581, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 1.2467 - sparse_categorical_accuracy: 0.7100 - val_loss: 1.1458 - val_sparse_categorical_accuracy: 0.7500\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2089 - sparse_categorical_accuracy: 0.7200\n",
      "Epoch 00018: val_loss improved from 1.14581 to 1.14180, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 1.2089 - sparse_categorical_accuracy: 0.7200 - val_loss: 1.1418 - val_sparse_categorical_accuracy: 0.6500\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1967 - sparse_categorical_accuracy: 0.6700\n",
      "Epoch 00019: val_loss improved from 1.14180 to 1.10124, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 1.1967 - sparse_categorical_accuracy: 0.6700 - val_loss: 1.1012 - val_sparse_categorical_accuracy: 0.6500\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1343 - sparse_categorical_accuracy: 0.6900\n",
      "Epoch 00020: val_loss did not improve from 1.10124\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1.1343 - sparse_categorical_accuracy: 0.6900 - val_loss: 1.1109 - val_sparse_categorical_accuracy: 0.7500\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1939 - sparse_categorical_accuracy: 0.6600\n",
      "Epoch 00021: val_loss improved from 1.10124 to 1.06861, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 1.1939 - sparse_categorical_accuracy: 0.6600 - val_loss: 1.0686 - val_sparse_categorical_accuracy: 0.6500\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1555 - sparse_categorical_accuracy: 0.6200\n",
      "Epoch 00022: val_loss did not improve from 1.06861\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 1.1555 - sparse_categorical_accuracy: 0.6200 - val_loss: 1.0787 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0732 - sparse_categorical_accuracy: 0.8100\n",
      "Epoch 00023: val_loss improved from 1.06861 to 1.01597, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 1.0732 - sparse_categorical_accuracy: 0.8100 - val_loss: 1.0160 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0682 - sparse_categorical_accuracy: 0.7100\n",
      "Epoch 00024: val_loss did not improve from 1.01597\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 1.0682 - sparse_categorical_accuracy: 0.7100 - val_loss: 1.0218 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0891 - sparse_categorical_accuracy: 0.8200\n",
      "Epoch 00025: val_loss improved from 1.01597 to 0.98023, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 1.0891 - sparse_categorical_accuracy: 0.8200 - val_loss: 0.9802 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0337 - sparse_categorical_accuracy: 0.7400\n",
      "Epoch 00026: val_loss did not improve from 0.98023\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 1.0337 - sparse_categorical_accuracy: 0.7400 - val_loss: 0.9833 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0395 - sparse_categorical_accuracy: 0.8200\n",
      "Epoch 00027: val_loss improved from 0.98023 to 0.94786, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 1.0395 - sparse_categorical_accuracy: 0.8200 - val_loss: 0.9479 - val_sparse_categorical_accuracy: 0.8500\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9925 - sparse_categorical_accuracy: 0.7500\n",
      "Epoch 00028: val_loss improved from 0.94786 to 0.94527, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 489ms/step - loss: 0.9925 - sparse_categorical_accuracy: 0.7500 - val_loss: 0.9453 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9859 - sparse_categorical_accuracy: 0.8600\n",
      "Epoch 00029: val_loss improved from 0.94527 to 0.91826, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 428ms/step - loss: 0.9859 - sparse_categorical_accuracy: 0.8600 - val_loss: 0.9183 - val_sparse_categorical_accuracy: 0.8500\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9666 - sparse_categorical_accuracy: 0.7600\n",
      "Epoch 00030: val_loss did not improve from 0.91826\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.9666 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.9286 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9743 - sparse_categorical_accuracy: 0.8200\n",
      "Epoch 00031: val_loss improved from 0.91826 to 0.89321, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 0.9743 - sparse_categorical_accuracy: 0.8200 - val_loss: 0.8932 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9149 - sparse_categorical_accuracy: 0.7700\n",
      "Epoch 00032: val_loss improved from 0.89321 to 0.88582, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 0.9149 - sparse_categorical_accuracy: 0.7700 - val_loss: 0.8858 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9523 - sparse_categorical_accuracy: 0.8700\n",
      "Epoch 00033: val_loss improved from 0.88582 to 0.84660, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 437ms/step - loss: 0.9523 - sparse_categorical_accuracy: 0.8700 - val_loss: 0.8466 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8973 - sparse_categorical_accuracy: 0.7500\n",
      "Epoch 00034: val_loss did not improve from 0.84660\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.8973 - sparse_categorical_accuracy: 0.7500 - val_loss: 0.8599 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8477 - sparse_categorical_accuracy: 0.9300\n",
      "Epoch 00035: val_loss improved from 0.84660 to 0.82399, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 0.8477 - sparse_categorical_accuracy: 0.9300 - val_loss: 0.8240 - val_sparse_categorical_accuracy: 0.8500\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8458 - sparse_categorical_accuracy: 0.7900\n",
      "Epoch 00036: val_loss did not improve from 0.82399\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.8458 - sparse_categorical_accuracy: 0.7900 - val_loss: 0.8251 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8438 - sparse_categorical_accuracy: 0.9200\n",
      "Epoch 00037: val_loss improved from 0.82399 to 0.79984, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 0.8438 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.7998 - val_sparse_categorical_accuracy: 0.8500\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8224 - sparse_categorical_accuracy: 0.8200\n",
      "Epoch 00038: val_loss did not improve from 0.79984\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.8224 - sparse_categorical_accuracy: 0.8200 - val_loss: 0.8040 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7803 - sparse_categorical_accuracy: 0.8900\n",
      "Epoch 00039: val_loss improved from 0.79984 to 0.77193, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 0.7803 - sparse_categorical_accuracy: 0.8900 - val_loss: 0.7719 - val_sparse_categorical_accuracy: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8001 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 00040: val_loss improved from 0.77193 to 0.76742, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 0.8001 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.7674 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8003 - sparse_categorical_accuracy: 0.8900\n",
      "Epoch 00041: val_loss improved from 0.76742 to 0.73853, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 0.8003 - sparse_categorical_accuracy: 0.8900 - val_loss: 0.7385 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7508 - sparse_categorical_accuracy: 0.8800\n",
      "Epoch 00042: val_loss did not improve from 0.73853\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.7508 - sparse_categorical_accuracy: 0.8800 - val_loss: 0.7476 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8077 - sparse_categorical_accuracy: 0.8700\n",
      "Epoch 00043: val_loss improved from 0.73853 to 0.72002, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 0.8077 - sparse_categorical_accuracy: 0.8700 - val_loss: 0.7200 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7620 - sparse_categorical_accuracy: 0.8700\n",
      "Epoch 00044: val_loss did not improve from 0.72002\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.7620 - sparse_categorical_accuracy: 0.8700 - val_loss: 0.7315 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7127 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00045: val_loss improved from 0.72002 to 0.68253, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 462ms/step - loss: 0.7127 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.6825 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7309 - sparse_categorical_accuracy: 0.8600\n",
      "Epoch 00046: val_loss did not improve from 0.68253\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.7309 - sparse_categorical_accuracy: 0.8600 - val_loss: 0.7211 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6983 - sparse_categorical_accuracy: 0.9200\n",
      "Epoch 00047: val_loss improved from 0.68253 to 0.66274, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 0.6983 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.6627 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6731 - sparse_categorical_accuracy: 0.9300\n",
      "Epoch 00048: val_loss did not improve from 0.66274\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.6731 - sparse_categorical_accuracy: 0.9300 - val_loss: 0.6789 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7401 - sparse_categorical_accuracy: 0.8700\n",
      "Epoch 00049: val_loss improved from 0.66274 to 0.66020, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.7401 - sparse_categorical_accuracy: 0.8700 - val_loss: 0.6602 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7076 - sparse_categorical_accuracy: 0.8800\n",
      "Epoch 00050: val_loss did not improve from 0.66020\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.7076 - sparse_categorical_accuracy: 0.8800 - val_loss: 0.6737 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6688 - sparse_categorical_accuracy: 0.8900\n",
      "Epoch 00051: val_loss improved from 0.66020 to 0.63941, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.6688 - sparse_categorical_accuracy: 0.8900 - val_loss: 0.6394 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6928 - sparse_categorical_accuracy: 0.9100\n",
      "Epoch 00052: val_loss improved from 0.63941 to 0.63884, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 0.6928 - sparse_categorical_accuracy: 0.9100 - val_loss: 0.6388 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6254 - sparse_categorical_accuracy: 0.9200\n",
      "Epoch 00053: val_loss improved from 0.63884 to 0.62960, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.6254 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.6296 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6516 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 00054: val_loss improved from 0.62960 to 0.61979, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.6516 - sparse_categorical_accuracy: 0.9000 - val_loss: 0.6198 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6518 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 00055: val_loss improved from 0.61979 to 0.61929, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 0.6518 - sparse_categorical_accuracy: 0.8500 - val_loss: 0.6193 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6183 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00056: val_loss improved from 0.61929 to 0.59160, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 0.6183 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.5916 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6061 - sparse_categorical_accuracy: 0.8900\n",
      "Epoch 00057: val_loss did not improve from 0.59160\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.6061 - sparse_categorical_accuracy: 0.8900 - val_loss: 0.6055 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6222 - sparse_categorical_accuracy: 0.9300\n",
      "Epoch 00058: val_loss improved from 0.59160 to 0.57847, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 0.6222 - sparse_categorical_accuracy: 0.9300 - val_loss: 0.5785 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6128 - sparse_categorical_accuracy: 0.9100\n",
      "Epoch 00059: val_loss did not improve from 0.57847\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.6128 - sparse_categorical_accuracy: 0.9100 - val_loss: 0.5801 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6368 - sparse_categorical_accuracy: 0.9100\n",
      "Epoch 00060: val_loss did not improve from 0.57847\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.6368 - sparse_categorical_accuracy: 0.9100 - val_loss: 0.5971 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6461 - sparse_categorical_accuracy: 0.8600\n",
      "Epoch 00061: val_loss improved from 0.57847 to 0.56056, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 438ms/step - loss: 0.6461 - sparse_categorical_accuracy: 0.8600 - val_loss: 0.5606 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5645 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00062: val_loss did not improve from 0.56056\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.5645 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.5914 - val_sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5544 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 00063: val_loss improved from 0.56056 to 0.53558, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 0.5544 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.5356 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6058 - sparse_categorical_accuracy: 0.8900\n",
      "Epoch 00064: val_loss did not improve from 0.53558\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.6058 - sparse_categorical_accuracy: 0.8900 - val_loss: 0.5521 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6049 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 00065: val_loss did not improve from 0.53558\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.6049 - sparse_categorical_accuracy: 0.8500 - val_loss: 0.5411 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5231 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00066: val_loss improved from 0.53558 to 0.51352, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 0.5231 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.5135 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5388 - sparse_categorical_accuracy: 0.9300\n",
      "Epoch 00067: val_loss did not improve from 0.51352\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.5388 - sparse_categorical_accuracy: 0.9300 - val_loss: 0.5294 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5629 - sparse_categorical_accuracy: 0.9300\n",
      "Epoch 00068: val_loss improved from 0.51352 to 0.50436, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 428ms/step - loss: 0.5629 - sparse_categorical_accuracy: 0.9300 - val_loss: 0.5044 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5304 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00069: val_loss did not improve from 0.50436\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.5304 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.5275 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5432 - sparse_categorical_accuracy: 0.9300\n",
      "Epoch 00070: val_loss improved from 0.50436 to 0.49038, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 0.5432 - sparse_categorical_accuracy: 0.9300 - val_loss: 0.4904 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5150 - sparse_categorical_accuracy: 0.9400\n",
      "Epoch 00071: val_loss did not improve from 0.49038\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.5150 - sparse_categorical_accuracy: 0.9400 - val_loss: 0.5071 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4944 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00072: val_loss improved from 0.49038 to 0.46677, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 0.4944 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.4668 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4702 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 00073: val_loss did not improve from 0.46677\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.4702 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.4889 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5106 - sparse_categorical_accuracy: 0.9200\n",
      "Epoch 00074: val_loss did not improve from 0.46677\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.5106 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.4724 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4689 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00075: val_loss improved from 0.46677 to 0.45857, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 0.4689 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.4586 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5093 - sparse_categorical_accuracy: 0.9100\n",
      "Epoch 00076: val_loss did not improve from 0.45857\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.5093 - sparse_categorical_accuracy: 0.9100 - val_loss: 0.4799 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4942 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00077: val_loss improved from 0.45857 to 0.43934, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 0.4942 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.4393 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4491 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00078: val_loss did not improve from 0.43934\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.4491 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.4540 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4915 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00079: val_loss improved from 0.43934 to 0.42777, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 0.4915 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.4278 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4396 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00080: val_loss improved from 0.42777 to 0.42688, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 0.4396 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.4269 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4252 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00081: val_loss did not improve from 0.42688\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.4252 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.4337 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4526 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00082: val_loss did not improve from 0.42688\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.4526 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.4287 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4354 - sparse_categorical_accuracy: 0.9300\n",
      "Epoch 00083: val_loss did not improve from 0.42688\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.4354 - sparse_categorical_accuracy: 0.9300 - val_loss: 0.4519 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4078 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 00084: val_loss improved from 0.42688 to 0.41984, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 0.4078 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.4198 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4716 - sparse_categorical_accuracy: 0.9400\n",
      "Epoch 00085: val_loss improved from 0.41984 to 0.41935, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 431ms/step - loss: 0.4716 - sparse_categorical_accuracy: 0.9400 - val_loss: 0.4193 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4374 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00086: val_loss improved from 0.41935 to 0.41563, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 1s 503ms/step - loss: 0.4374 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.4156 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4148 - sparse_categorical_accuracy: 0.9200\n",
      "Epoch 00087: val_loss improved from 0.41563 to 0.40920, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 0.4148 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.4092 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4356 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00088: val_loss did not improve from 0.40920\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.4356 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.4141 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4100 - sparse_categorical_accuracy: 0.9400\n",
      "Epoch 00089: val_loss improved from 0.40920 to 0.38836, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 0.4100 - sparse_categorical_accuracy: 0.9400 - val_loss: 0.3884 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3832 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00090: val_loss improved from 0.38836 to 0.38360, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 0.3832 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.3836 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4046 - sparse_categorical_accuracy: 0.9400\n",
      "Epoch 00091: val_loss improved from 0.38360 to 0.37421, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 0.4046 - sparse_categorical_accuracy: 0.9400 - val_loss: 0.3742 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3853 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 00092: val_loss did not improve from 0.37421\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.3853 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.3778 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4106 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00093: val_loss improved from 0.37421 to 0.35894, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 0.4106 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.3589 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3767 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 00094: val_loss improved from 0.35894 to 0.35644, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 0.3767 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.3564 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3687 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00095: val_loss did not improve from 0.35644\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.3687 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.3653 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3669 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00096: val_loss improved from 0.35644 to 0.35115, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 0.3669 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.3512 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3612 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00097: val_loss did not improve from 0.35115\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.3612 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.3636 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3887 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 00098: val_loss did not improve from 0.35115\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.3887 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.3558 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3360 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00099: val_loss improved from 0.35115 to 0.34951, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 483ms/step - loss: 0.3360 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3495 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3584 - sparse_categorical_accuracy: 0.9400\n",
      "Epoch 00100: val_loss did not improve from 0.34951\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.3584 - sparse_categorical_accuracy: 0.9400 - val_loss: 0.3545 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3258 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00101: val_loss improved from 0.34951 to 0.33248, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 0.3258 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.3325 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3304 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00102: val_loss improved from 0.33248 to 0.33028, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 0.3304 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.3303 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3814 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 00103: val_loss did not improve from 0.33028\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.3814 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.3594 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3590 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 00104: val_loss improved from 0.33028 to 0.32375, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 0.3590 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.3238 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3030 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00105: val_loss improved from 0.32375 to 0.32030, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 0.3030 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3203 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3217 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00106: val_loss did not improve from 0.32030\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.3217 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.3260 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2994 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 00107: val_loss improved from 0.32030 to 0.31587, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 0.2994 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.3159 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2987 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00108: val_loss did not improve from 0.31587\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.2987 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.3274 - val_sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3157 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 00109: val_loss improved from 0.31587 to 0.29585, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 0.3157 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.2958 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2874 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00110: val_loss did not improve from 0.29585\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.2874 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.3244 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2895 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00111: val_loss improved from 0.29585 to 0.29580, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 0.2895 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2958 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2837 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 00112: val_loss did not improve from 0.29580\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.2837 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.3076 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3019 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00113: val_loss improved from 0.29580 to 0.29406, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 0.3019 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.2941 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2727 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 00114: val_loss improved from 0.29406 to 0.28781, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 0.2727 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.2878 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3152 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00115: val_loss improved from 0.28781 to 0.28288, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.3152 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.2829 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2998 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00116: val_loss did not improve from 0.28288\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.2998 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.3028 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3381 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00117: val_loss did not improve from 0.28288\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.3381 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.2876 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2813 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 00118: val_loss improved from 0.28288 to 0.27798, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 456ms/step - loss: 0.2813 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.2780 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2994 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00119: val_loss improved from 0.27798 to 0.27282, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 455ms/step - loss: 0.2994 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.2728 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2831 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00120: val_loss improved from 0.27282 to 0.26370, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 0.2831 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.2637 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2754 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00121: val_loss did not improve from 0.26370\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.2754 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.2683 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2525 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00122: val_loss improved from 0.26370 to 0.24664, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 0.2525 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.2466 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2481 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00123: val_loss did not improve from 0.24664\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.2481 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.2604 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2729 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00124: val_loss did not improve from 0.24664\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.2729 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.2500 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2893 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00125: val_loss did not improve from 0.24664\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2893 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.2835 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2552 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 00126: val_loss improved from 0.24664 to 0.23703, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 0.2552 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.2370 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2858 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00127: val_loss did not improve from 0.23703\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2858 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.2460 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2679 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 00128: val_loss improved from 0.23703 to 0.23585, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 0.2679 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.2359 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2625 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00129: val_loss did not improve from 0.23585\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2625 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.2521 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2233 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00130: val_loss improved from 0.23585 to 0.22204, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 0.2233 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2220 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2829 - sparse_categorical_accuracy: 0.9400\n",
      "Epoch 00131: val_loss did not improve from 0.22204\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.2829 - sparse_categorical_accuracy: 0.9400 - val_loss: 0.2495 - val_sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2422 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00132: val_loss improved from 0.22204 to 0.21676, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.2422 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.2168 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2365 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 00133: val_loss did not improve from 0.21676\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2365 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.2273 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2599 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 00134: val_loss did not improve from 0.21676\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2599 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.2241 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2463 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00135: val_loss improved from 0.21676 to 0.21365, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 0.2463 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.2136 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2207 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00136: val_loss improved from 0.21365 to 0.21230, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.2207 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.2123 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2534 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00137: val_loss did not improve from 0.21230\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.2534 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.2181 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2408 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00138: val_loss improved from 0.21230 to 0.19993, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 454ms/step - loss: 0.2408 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.1999 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2021 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 00139: val_loss did not improve from 0.19993\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.2021 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.2091 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2313 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00140: val_loss did not improve from 0.19993\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.2313 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2142 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2276 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 00141: val_loss did not improve from 0.19993\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 0.2276 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.2139 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2444 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00142: val_loss improved from 0.19993 to 0.19767, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 0.2444 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.1977 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2050 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00143: val_loss did not improve from 0.19767\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.2050 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.2094 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2466 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 00144: val_loss improved from 0.19767 to 0.19300, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.2466 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.1930 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2248 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 00145: val_loss did not improve from 0.19300\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.2248 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1992 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2376 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00146: val_loss improved from 0.19300 to 0.18457, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 0.2376 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.1846 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2162 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00147: val_loss did not improve from 0.18457\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.2162 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.1958 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2365 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00148: val_loss did not improve from 0.18457\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.2365 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.1919 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2166 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 00149: val_loss did not improve from 0.18457\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.2166 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.2127 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2116 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00150: val_loss improved from 0.18457 to 0.17696, saving model to ./model/recycle_vgg_pic.h5\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 0.2116 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.1770 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2140 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00151: val_loss did not improve from 0.17696\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.2140 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.2400 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2335 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 00152: val_loss did not improve from 0.17696\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.2335 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.1907 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2107 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 00153: val_loss did not improve from 0.17696\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.2107 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.1905 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2170 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 00154: val_loss did not improve from 0.17696\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.2170 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.1797 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1837 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 00155: val_loss did not improve from 0.17696\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.1837 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.1820 - val_sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00155: early stopping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    model_base = VGG16(weights='imagenet',\n",
    "                       include_top=False,\n",
    "                       input_shape=(299,299,3))\n",
    "\n",
    "    model_base.trainable = False  # Convolution Layer 동결\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(model_base)\n",
    "\n",
    "    model.add(Flatten(input_shape=(9*9*512,)))\n",
    "    \n",
    "\n",
    "    model.add(Dense(9,\n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=RMSprop(learning_rate=1e-4),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    # Model saving callback\n",
    "    checkpointer = ModelCheckpoint(filepath='./model/recycle_vgg_pic.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "#     Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=5)\n",
    "\n",
    "    epochs = 500\n",
    "    history = model.fit_generator(\n",
    "        trainGen, \n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpointer, early_stopping],\n",
    "        steps_per_epoch=1, \n",
    "        validation_data=validationGen,\n",
    "        validation_steps=1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "\n",
    "plt.plot(train_acc,'bo', color='r', label='accuracy')\n",
    "plt.plot(val_acc,'b', color='b', label='val accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(train_loss,'bo', color='r', label='train loss')\n",
    "plt.plot(val_loss,'b', color='b', label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3,3,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Add]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6374dabaf116>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mvgg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    180\u001b[0m     if (h5py is not None and (\n\u001b[1;32m    181\u001b[0m         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 182\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[0;32m--> 178\u001b[0;31m                                                custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    356\u001b[0m             custom_objects=dict(\n\u001b[1;32m    357\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                 list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    359\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_config\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       layer = layer_module.deserialize(layer_config,\n\u001b[0;32m--> 487\u001b[0;31m                                        custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    488\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     if (not model.inputs and build_input_shape and\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    356\u001b[0m             custom_objects=dict(\n\u001b[1;32m    357\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                 list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    359\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \"\"\"\n\u001b[1;32m    631\u001b[0m     input_tensors, output_tensors, created_layers = reconstruct_from_config(\n\u001b[0;32m--> 632\u001b[0;31m         config, custom_objects)\n\u001b[0m\u001b[1;32m    633\u001b[0m     model = cls(inputs=input_tensors, outputs=output_tensors,\n\u001b[1;32m    634\u001b[0m                 name=config.get('name'))\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mreconstruct_from_config\u001b[0;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[1;32m   1227\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m           \u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m   \u001b[0minput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_tensors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m       \u001b[0minput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnest_if_single_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m       \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       \u001b[0;31m# Update node index map.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 940\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2686\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       self.bias = self.add_weight(\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    617\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                         shape=None):\n\u001b[1;32m    198\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2598\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m     return variables.RefVariable(\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1516\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m   def _init_from_args(self,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1649\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1651\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1653\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/initializers/initializers_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    395\u001b[0m        \u001b[0;34m(\u001b[0m\u001b[0mvia\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_floatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVarianceScaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_get_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1042\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     return op(\n\u001b[0;32m-> 1044\u001b[0;31m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    305\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;31m# TODO(b/132092188): C++ shape inference inside functional ops does not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;31m# cross FuncGraph boundaries since that information is only available in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    337\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3,3,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Add]"
     ]
    }
   ],
   "source": [
    "#### from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns \n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "model_path = './model/recycle_vgg_pic.h5'\n",
    "\n",
    "\n",
    "vgg_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "\n",
    "x_train, y_train= validationGen.next()\n",
    "print(y_train)\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = vgg_model.predict_generator(validationGen, 1)\n",
    "\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "\n",
    "print('Confusion Matrix')\n",
    "# display(confusion_matrix(validationGen.classes, y_pred))\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))  # inch단위로 그림의 크기\n",
    "sns.heatmap(\n",
    "    confusion_matrix(y_train, y_pred),\n",
    "    annot = True, # 숫자표현\n",
    "    fmt = '3d',    # 정수표현     \n",
    "    cmap = 'Blues', # color 색상\n",
    "    ax = ax,       # 그래프로 사용할 subplot\n",
    "    xticklabels=category_csv['bottle_name'], \n",
    "    yticklabels=category_csv['bottle_name']\n",
    ")\n",
    "ax.set_xlabel('Predict')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_latest_p37] *",
   "language": "python",
   "name": "conda-env-tensorflow2_latest_p37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
